{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequent pattern mining - Exercise 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2-1 - Combinatorial Explosion  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A: A database contains transactions over the following items: “apples”, “bananas”, and “cherries”. How many different combinations of these items can exist (i.e., how many different transactions could possibly occur in the database)?**  \n",
    "\n",
    "This can be solved with the formula 2^n which is 2^3 in this case, which yields 8.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "['apple']\n",
      "['cherry']\n",
      "['apple', 'cherry']\n",
      "['banana']\n",
      "['apple', 'banana']\n",
      "['cherry', 'banana']\n",
      "['apple', 'cherry', 'banana']\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "def get_powerset(s, verbose = False):\n",
    "    x = len(s)\n",
    "\n",
    "    return_set = []\n",
    "    for i in range(1 << x):\n",
    "        sub_set = [s[j] for j in range(x) if (i & (1 << j))]\n",
    "        return_set.append(sub_set)\n",
    "        if verbose:\n",
    "            print(sub_set)\n",
    "    return return_set\n",
    "\n",
    "def get_powerset_length(x):\n",
    "    \"\"\" \n",
    "    Formula is 2**n where n is the length of the set\n",
    "    \"\"\"\n",
    "    return 2**len(x)\n",
    "\n",
    "\n",
    "my_set = [\"apple\", \"cherry\", \"banana\"]\n",
    "power_set = get_powerset(my_set, verbose=True)\n",
    "print(get_powerset_length(my_set))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B: The database now also contains the items “dates”, “eggplants”, “figs”, and “guavas”. How many possible transactions do we have now?**   \n",
    "\n",
    "Again now the formel is 2^6, which yields 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "my_set = [\"apple\", \"cherry\", \"banana\", \"dates\", \"eggplants\", \"figs\"]\n",
    "#power_set = get_powerset(my_set)\n",
    "print(get_powerset_length(my_set))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C: How many combinations (possible different transactions) do we have with n items?**  \n",
    "\n",
    "Again it is just 2^n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**D: How many transactions with exactly two items (i.e., 2-itemsets) can we have when the database contains\n",
    "3 items? When it contains 5 items? How many k-itemsets do we have when the database contains n\n",
    "items?**  \n",
    "\n",
    "This is calculated with the binomial coefficent formula, which is n! / ( k! * (n - k)! )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0\n",
      "3.0\n"
     ]
    }
   ],
   "source": [
    "def get_k_itemsets(power_set, k, verbose = False):\n",
    "    return_set = []\n",
    "    for sub_set in power_set:\n",
    "        if len(sub_set) == k:\n",
    "            return_set.append(sub_set)\n",
    "            if verbose:\n",
    "                print(sub_set)\n",
    "    return return_set\n",
    "\n",
    "\n",
    "from math import factorial\n",
    "def get_len_of_k_itemsets(n, k):\n",
    "    \"\"\"\n",
    "    Calcualtes how many k itemsets of length k there will be in the superset\n",
    "    n can either be list of items or an integer, which indicates the length of the itemset\n",
    "    \"\"\"\n",
    "    if isinstance(n, list):\n",
    "        n = len(n)\n",
    "    else:\n",
    "        n = n\n",
    "\n",
    "    if k > n:\n",
    "        return 0\n",
    "    else:\n",
    "        return factorial(n) / (factorial(k) * factorial(n - k)) \n",
    "\n",
    "my_set = [\"apple\", \"cherry\", \"banana\", \"dates\", \"eggplants\"]\n",
    "print(get_len_of_k_itemsets(my_set, 2))\n",
    "\n",
    "\n",
    "my_set = [\"apple\", \"cherry\", \"banana\"]\n",
    "print(get_len_of_k_itemsets(my_set, 2))\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-2: Itemsets and Association Rules\n",
    "\n",
    "Given a set of transactions T according to the following table:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>items_in_basket</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[milk, beer, diapers]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[bread, butter, milk]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[milk, diapers, cookies]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[bread, butter, cookies]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[beer, cookies, diapers]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[milk, diapers, bread, butter]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[bread, butter, diapers]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[beer, diapers]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[milk, diapers, bread, butter]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[beer, cookies]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  items_in_basket\n",
       "0           [milk, beer, diapers]\n",
       "1           [bread, butter, milk]\n",
       "2        [milk, diapers, cookies]\n",
       "3        [bread, butter, cookies]\n",
       "4        [beer, cookies, diapers]\n",
       "5  [milk, diapers, bread, butter]\n",
       "6        [bread, butter, diapers]\n",
       "7                 [beer, diapers]\n",
       "8  [milk, diapers, bread, butter]\n",
       "9                 [beer, cookies]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(\n",
    "    {\"items_in_basket\": [[\"milk\", \"beer\", \"diapers\"],\n",
    "                         [\"bread\", \"butter\", \"milk\"],\n",
    "                         [\"milk\", \"diapers\", \"cookies\"],\n",
    "                         [\"bread\", \"butter\", \"cookies\"],\n",
    "                         [\"beer\", \"cookies\", \"diapers\"],\n",
    "                         [\"milk\", \"diapers\", \"bread\", \"butter\"],\n",
    "                         [\"bread\", \"butter\", \"diapers\"],\n",
    "                         [\"beer\", \"diapers\"],\n",
    "                         [\"milk\", \"diapers\", \"bread\", \"butter\"],\n",
    "                         [\"beer\", \"cookies\"]]}\n",
    ")\n",
    "\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A: What are the support and the confidence of {milk} -> {diapers}**  \n",
    "\n",
    "The support is retrieved by counting how often all the components (ie. milk and diapers) occur in the same transaction. In this case 4 times.  \n",
    "\n",
    "The confidence is retrieved by by dividing the support of the association rule with the support of the antecetent (left side). which in this case is 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support: 4\n",
      "Confidence: 0.8\n"
     ]
    }
   ],
   "source": [
    "transactions = list(df[\"items_in_basket\"])\n",
    "\n",
    "def get_support(transactions: list[str], antecedent: list[str], consequent: list[str] = [], fraction: bool = False):\n",
    "    \"\"\"\n",
    "    Takes a list of transactions, a antecedent (left of of the rule) and a consequent (right part of the rule)\n",
    "    returns the support of the rule.\n",
    "    The support is retrieved by counting how often all the components (ie. milk and diapers) occur in the same transaction\n",
    "    If you only wish to get the support of an itemset, just leave consequent blank.\n",
    "    \"\"\"\n",
    "    association_rule = set(consequent + antecedent)\n",
    "    support = 0\n",
    "    for transaction in transactions:\n",
    "        transaction = set(transaction)\n",
    "        if set.intersection(association_rule, transaction) == association_rule:\n",
    "            support += 1\n",
    "    if fraction:\n",
    "        support = support / len(transactions)\n",
    "    return support\n",
    "\n",
    "\n",
    "\n",
    "def get_confidence(transactions: list[str], antecedent: list[str], consequent: list[str] = []):\n",
    "    \"\"\"\n",
    "    Takes a list of transactions, a antecedent (left of of the rule) and a consequent (right part of the rule)\n",
    "    returns the confidence of the rule\n",
    "    \n",
    "    The formula is given by dividing the support of the rule with the support of the antecedent\n",
    "    \"\"\"\n",
    "    return get_support(transactions, antecedent, consequent) / get_support(transactions, antecedent)\n",
    "\n",
    "\n",
    "support = get_support(transactions, antecedent = [\"milk\"], consequent = [\"diapers\"])\n",
    "confidence = get_confidence(transactions, antecedent = [\"milk\"], consequent = [\"diapers\"])\n",
    "\n",
    "print(f\"Support: {support}\")\n",
    "print(f\"Confidence: {confidence}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B: What are the support and confidewnce of {diapers} -> {milk}?**  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support: 4\n",
      "Confidence: 0.5714285714285714\n"
     ]
    }
   ],
   "source": [
    "support = get_support(transactions, antecedent = [\"diapers\"], consequent = [\"milk\"])\n",
    "confidence = get_confidence(transactions, antecedent = [\"diapers\"], consequent = [\"milk\"])\n",
    "\n",
    "print(f\"Support: {support}\")\n",
    "print(f\"Confidence: {confidence}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C: What is the maximum number of size-3 itemsets that can be derived from this data set?**  \n",
    "\n",
    "This is done by finding the transactions with the most number of items and calculating how many subsets it can contain, (the binomial coefficent). Here we will use the function from previously\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum number of 3-itemsets: 4.0\n"
     ]
    }
   ],
   "source": [
    "def find_max_list(list: list[str]):\n",
    "    \"\"\"\n",
    "    This finds the length of the longest list in a list of list.\n",
    "    It does NOT work with multiple nested lists.\n",
    "    \"\"\"\n",
    "    return max([len(i) for i in list])\n",
    "\n",
    "max_itemsets = get_len_of_k_itemsets(n = find_max_list(transactions),\n",
    "                                     k = 3)\n",
    "print(f\"Maximum number of 3-itemsets: {max_itemsets}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**D: What is the maximum number of association rules that can be extracted from this dataset (including rules that have zero support)?**  \n",
    " \n",
    " The formula is from the book data mining second edition Tan. ( (3^r) - (2^(r + 1)) ) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max association rules are: 602\n"
     ]
    }
   ],
   "source": [
    "def get_unique_items(transactions):\n",
    "    unique_items = []\n",
    "    for transaction in transactions:\n",
    "        for item in transaction:\n",
    "            if not (item in unique_items):\n",
    "                unique_items.append(item)\n",
    "    return unique_items    \n",
    "\n",
    "def get_max_association_rules(transactions: list[list[str]] | int):\n",
    "    \"\"\"\n",
    "    Returns the maximum amount of association rules, that are possible to generate, without empty sets\n",
    "    it uses the fomula ( (3**r) - (2**(r + 1)) ) + 1, where r is the amount of unique items in the database\n",
    "    it does not consider empty sets.\n",
    "    \"\"\"\n",
    "    if isinstance(transactions, list):\n",
    "        unique_items = get_unique_items(transactions)\n",
    "        r = len(unique_items)\n",
    "    else:\n",
    "        r = transactions\n",
    "    return ( (3**r) - (2**(r + 1)) ) + 1\n",
    "\n",
    "print(f\"Max association rules are: {get_max_association_rules(transactions)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**E: What is the maximum size of the frequent itemsets that can be extracted (assuming support > 0 )?**\n",
    "\n",
    "The maximum support is 4, since the itemset {milk, diapers, bread, butter} occurs once and has a support of 2\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**F: Find an itemset (of size 2 or larger) that has the largest support**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"['G', 'B']\": 8,\n",
       " \"['H', 'E']\": 7,\n",
       " \"['H', 'B']\": 7,\n",
       " \"['E', 'B']\": 7,\n",
       " \"['B', 'A']\": 6,\n",
       " \"['H', 'G']\": 5,\n",
       " \"['H', 'A']\": 5,\n",
       " \"['E', 'G']\": 5,\n",
       " \"['E', 'A']\": 5,\n",
       " \"['E', 'C']\": 5,\n",
       " \"['E', 'F']\": 5,\n",
       " \"['H', 'C']\": 4,\n",
       " \"['H', 'F']\": 4,\n",
       " \"['G', 'A']\": 4,\n",
       " \"['G', 'D']\": 4,\n",
       " \"['B', 'F']\": 4,\n",
       " \"['B', 'D']\": 4,\n",
       " \"['C', 'F']\": 4,\n",
       " \"['G', 'F']\": 3,\n",
       " \"['B', 'C']\": 3,\n",
       " \"['A', 'C']\": 3,\n",
       " \"['H', 'D']\": 2,\n",
       " \"['H', 'K']\": 2,\n",
       " \"['E', 'K']\": 2,\n",
       " \"['G', 'C']\": 2,\n",
       " \"['B', 'K']\": 2,\n",
       " \"['A', 'F']\": 2,\n",
       " \"['A', 'D']\": 2,\n",
       " \"['F', 'D']\": 2,\n",
       " \"['H', 'L']\": 1,\n",
       " \"['H', 'I']\": 1,\n",
       " \"['E', 'D']\": 1,\n",
       " \"['E', 'L']\": 1,\n",
       " \"['E', 'I']\": 1,\n",
       " \"['G', 'L']\": 1,\n",
       " \"['G', 'K']\": 1,\n",
       " \"['G', 'I']\": 1,\n",
       " \"['B', 'L']\": 1,\n",
       " \"['B', 'I']\": 1,\n",
       " \"['A', 'K']\": 1,\n",
       " \"['C', 'D']\": 1,\n",
       " \"['C', 'L']\": 1,\n",
       " \"['F', 'L']\": 1,\n",
       " \"['F', 'K']\": 1,\n",
       " \"['F', 'I']\": 1,\n",
       " \"['D', 'L']\": 1,\n",
       " \"['K', 'I']\": 1,\n",
       " \"['A', 'L']\": 0,\n",
       " \"['A', 'I']\": 0,\n",
       " \"['C', 'K']\": 0,\n",
       " \"['C', 'I']\": 0,\n",
       " \"['D', 'K']\": 0,\n",
       " \"['D', 'I']\": 0,\n",
       " \"['L', 'K']\": 0,\n",
       " \"['L', 'I']\": 0}"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "from itertools import permutations\n",
    "def get_itemset_with_highest_support(transactions: list[list[str]], size: int | None = None):\n",
    "    \"\"\"\n",
    "    FIXME\n",
    "    \"\"\"\n",
    "    unique_items = get_unique_items(transactions)\n",
    "    if size:\n",
    "        combs = [list(comb) for comb in combinations(unique_items, size)]\n",
    "    else:\n",
    "        combs = [list(comb) for comb in combinations(unique_items)]\n",
    "\n",
    "    support = {}\n",
    "    for comb in combs:\n",
    "        support[str(comb)] = get_support(transactions, comb)\n",
    "\n",
    "    return dict(sorted(support.items(), key=lambda item: item[1], reverse=True))\n",
    "    #power_set = get_powerset(get_unique_items(transactions))\n",
    "    #print(power_set)\n",
    "\n",
    "get_itemset_with_highest_support(transactions, size = 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**G: Find a pair of items a and b such that the rule {a} -> {b} and {b} -> {a} have the same confidence**  \n",
    "\n",
    "How to do this is simple: FInd items, who has the same support, and then test them together until you find some with same confidence. But remember, confidence is not always symetrical, it is possible to have different values depending on which is antecedent and consequent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Milk and bread conf: 0.6 \n",
      "Bread and milk conf: 0.6 \n"
     ]
    }
   ],
   "source": [
    "print(f\"Milk and bread conf: {get_confidence(transactions, ['milk'], ['bread'])} \")\n",
    "\n",
    "print(f'Bread and milk conf: {get_confidence(transactions, [\"bread\"], [\"milk\"])} ')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-3 Apriori candidate generation  \n",
    "Given the frequent 3-itemsets:  \n",
    "\n",
    "    {1, 2, 3}, {1, 2, 4}, {1, 2, 5}, {1, 3, 4}, {1, 3, 5}, {2, 3, 4}, {2, 3, 5}, {3, 4, 5}  \n",
    "    \n",
    "List all candidate 4-itemsets following the Apriori joining and pruning procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{1, 2, 3, 4}, {1, 2, 3, 5}, {1, 2, 4, 5}, {1, 3, 4, 5}, {2, 3, 4, 5}]\n"
     ]
    }
   ],
   "source": [
    "def _apriori_join(items):\n",
    "    \n",
    "    candidates = []\n",
    "    for i, itemset_outer in enumerate(items):\n",
    "        itemset_outer = sorted(list(itemset_outer))\n",
    "        \n",
    "        for j, itemset_inner in enumerate(items):\n",
    "            itemset_inner = sorted(list(itemset_inner))\n",
    "            if j != i and itemset_inner[0: - 1] == itemset_outer[0: - 1]:\n",
    "                candidate = set(itemset_inner + itemset_outer)\n",
    "                if candidate not in candidates:\n",
    "                    candidates.append(set(itemset_inner + itemset_outer))\n",
    "    return candidates                    \n",
    "\n",
    "\n",
    "def _apriori_prune(candidates, items, k):\n",
    "    \n",
    "    pruned_candidates = []\n",
    "    for i, candidate in enumerate(candidates):\n",
    "        subsets = [set(sorted(subset)) for subset in combinations(candidate, k)]\n",
    "        for subset in subsets:\n",
    "            valid_subset = False\n",
    "            \n",
    "            for item in items:\n",
    "                if item.issubset(candidate):\n",
    "                    valid_subset = True\n",
    "                    break\n",
    "\n",
    "        \n",
    "        if valid_subset:\n",
    "            pruned_candidates.append(candidate)\n",
    "    return pruned_candidates\n",
    "\n",
    "\n",
    "def apriori_generate_candidates(items: list[set[str | int]]):\n",
    "    \"\"\"\n",
    "    Public function for generating apriori candidates\n",
    "    \"\"\"\n",
    "\n",
    "    n = len(items[0])\n",
    "    candidates = _apriori_join(items)\n",
    "    pruned_candidates = _apriori_prune(candidates, items, n)\n",
    "    return pruned_candidates\n",
    "\n",
    "\n",
    "def aprio_algo(itemset, support):\n",
    "    \n",
    "    \n",
    "    for i in range(len(itemset)):\n",
    "        itemset[i] = set(sorted(itemset[i]))\n",
    "    unique_items = get_unique_items(itemset)\n",
    "    \n",
    "    \n",
    "    s_1 = []\n",
    "    for item in unique_items:\n",
    "        if get_support(itemset, [item]) > support:\n",
    "            item =item\n",
    "            s_1.append(item)\n",
    "    \n",
    "    print(s_1)        \n",
    "    candidates = [set(subset) for subset in combinations(s_1, 2)]\n",
    "    \n",
    "    pruned_candidates = []\n",
    "    for candidate in candidates:\n",
    "        \n",
    "        count = 0\n",
    "        for item in itemset:\n",
    "            if candidate.issubset(item):\n",
    "                \n",
    "                count += 1 \n",
    "        \n",
    "        if count > support:\n",
    "            pruned_candidates.append(candidate)\n",
    "    \n",
    "    \n",
    "    candidates = pruned_candidates\n",
    "    print(candidates)\n",
    "    while len(candidates) > 0:\n",
    "        candidates = apriori_generate_candidates(candidates)\n",
    "        pruned_candidates = []\n",
    "        for candidate in candidates:\n",
    "            count = 0\n",
    "            for item in itemset:\n",
    "                if candidate.issubset(item):\n",
    "                    count += 1 \n",
    "            if count > support:\n",
    "                pruned_candidates.append(candidate)\n",
    "                \n",
    "                \n",
    "        print(candidates)\n",
    "        candidates = pruned_candidates\n",
    "        print(candidates)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "print(apriori_generate_candidates([{1, 2, 3}, {1, 2, 4}, {1, 2, 5}, {1, 3, 4}, {1, 3, 5}, {2, 3, 4}, {2, 3, 5}, {3, 4, 5}  ]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Title](imgs/Screenshot_4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'E', 'B', 'D', 'F', 'C']\n",
      "[{'A', 'E'}, {'A', 'B'}, {'A', 'C'}, {'B', 'E'}, {'C', 'E'}, {'D', 'B'}, {'C', 'B'}, {'D', 'C'}, {'F', 'C'}]\n",
      "[{'A', 'E', 'B'}, {'A', 'C', 'E'}, {'A', 'C', 'B'}, {'D', 'E', 'B'}, {'C', 'E', 'B'}, {'D', 'C', 'E'}, {'F', 'C', 'E'}, {'D', 'C', 'B'}, {'D', 'F', 'C'}]\n",
      "[{'A', 'C', 'E'}, {'C', 'E', 'B'}]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "transactions = [{\"A\", \"B\", \"E\"},\n",
    "                {\"B\", \"D\"},\n",
    "                {\"C\", \"D\", \"F\"},\n",
    "                {\"A\", \"B\", \"D\"},\n",
    "                {\"A\", \"C\", \"E\"},\n",
    "                {\"B\", \"C\", \"E\", \"F\"},\n",
    "                {\"A\", \"C\", \"E\"},\n",
    "                {\"A\", \"B\",\"C\", \"E\"},\n",
    "                {\"A\", \"B\", \"C\", \"D\", \"F\"},\n",
    "                {\"B\", \"C\", \"D\", \"E\"},\n",
    "                ]\n",
    "\n",
    "aprio_algo(transactions, support = 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FULL apriori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'H'}, {'E'}, {'G'}, {'B'}]\n",
      "[{'H', 'E'}, {'H', 'G'}, {'H', 'B'}, {'G', 'E'}, {'E', 'B'}, {'G', 'B'}]\n",
      "[{'H', 'G', 'E'}, {'H', 'E', 'B'}, {'H', 'G', 'B'}, {'E', 'G', 'B'}]\n",
      "[{'H', 'E', 'G', 'B'}]\n"
     ]
    }
   ],
   "source": [
    "def _apriori_join(items):\n",
    "    \n",
    "    candidates = []\n",
    "    for i, itemset_outer in enumerate(items):\n",
    "        itemset_outer = sorted(list(itemset_outer))\n",
    "        \n",
    "        for j, itemset_inner in enumerate(items):\n",
    "            itemset_inner = sorted(list(itemset_inner))\n",
    "            if j != i and itemset_inner[0: - 1] == itemset_outer[0: - 1]:\n",
    "                candidate = set(itemset_inner + itemset_outer)\n",
    "                if candidate not in candidates:\n",
    "                    candidates.append(set(itemset_inner + itemset_outer))\n",
    "    return candidates                    \n",
    "\n",
    "\n",
    "def _apriori_prune(candidates, items, k):\n",
    "    \n",
    "    pruned_candidates = []\n",
    "    for i, candidate in enumerate(candidates):\n",
    "        subsets = [set(sorted(subset)) for subset in combinations(candidate, k)]\n",
    "        for subset in subsets:\n",
    "            valid_subset = False\n",
    "            \n",
    "            for item in items:\n",
    "                if item.issubset(candidate):\n",
    "                    valid_subset = True\n",
    "                    break\n",
    "\n",
    "        \n",
    "        if valid_subset:\n",
    "            pruned_candidates.append(candidate)\n",
    "    return pruned_candidates\n",
    "\n",
    "\n",
    "def apriori_generate_candidates(items: list[set[str | int]]):\n",
    "    \"\"\"\n",
    "    Public function for generating apriori candidates\n",
    "    \"\"\"\n",
    "\n",
    "    n = len(items[0])\n",
    "    candidates = _apriori_join(items)\n",
    "    pruned_candidates = _apriori_prune(candidates, items, n)\n",
    "    return pruned_candidates\n",
    "\n",
    "\n",
    "def aprio_algo(itemset, support):\n",
    "    \n",
    "    \n",
    "    for i in range(len(itemset)):\n",
    "        itemset[i] = set(sorted(itemset[i]))\n",
    "    unique_items = get_unique_items(itemset)\n",
    "    \n",
    "    \n",
    "    s_1 = []\n",
    "    for item in unique_items:\n",
    "        if get_support(itemset, [item]) > support:\n",
    "            item =item\n",
    "            s_1.append(item)\n",
    "            \n",
    "    candidates = [set(subset) for subset in combinations(s_1, 2)]\n",
    "    \n",
    "    \n",
    "    pruned_candidates = []\n",
    "    for candidate in candidates:\n",
    "        \n",
    "        count = 0\n",
    "        for item in itemset:\n",
    "            if candidate.issubset(item):\n",
    "                \n",
    "                count += 1 \n",
    "        \n",
    "        if count > support:\n",
    "            pruned_candidates.append(candidate)\n",
    "    \n",
    "    \n",
    "    candidates = pruned_candidates\n",
    "    while len(candidates) > 0:\n",
    "        prev_candidates = candidates\n",
    "        candidates = apriori_generate_candidates(candidates)\n",
    "        pruned_candidates = []\n",
    "        for candidate in candidates:\n",
    "            count = 0\n",
    "            for item in itemset:\n",
    "                if candidate.issubset(item):\n",
    "                    count += 1 \n",
    "            if count > support:\n",
    "                pruned_candidates.append(candidate)\n",
    "                \n",
    "                \n",
    "        \n",
    "        candidates = pruned_candidates\n",
    "    return prev_candidates\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "transactions = [{\"B\", \"E\", \"G\", \"H\"},\n",
    "                {\"A\", \"B\", \"C\", \"E\", \"G\", \"H\"},\n",
    "                {\"A\", \"B\", \"C\", \"E\", \"F\", \"H\"},\n",
    "                {\"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"L\"},\n",
    "                {\"A\", \"B\", \"E\", \"K\", \"H\"},\n",
    "                {\"B\", \"E\", \"F\", \"G\", \"H\", \"I\", \"K\"},\n",
    "                {\"A\", \"B\", \"D\", \"G\", \"H\"},\n",
    "                {\"A\", \"B\", \"D\", \"G\"},\n",
    "                {\"B\", \"D\", \"F\", \"G\"},\n",
    "                {\"C\", \"E\", \"F\"},\n",
    "                {\"A\", \"C\", \"E\", \"F\", \"H\"},\n",
    "                {\"A\", \"B\", \"E\", \"G\"}\n",
    "                ]\n",
    "\n",
    "freq_items = aprio_algo(transactions, support = 3)\n",
    "\n",
    "\n",
    "for freq_item in freq_items:\n",
    "    freq_item = list(freq_item)\n",
    "    \n",
    "    for i in range(len(freq_item)):\n",
    "        i = i + 1\n",
    "        subsets = [set(sorted(subset)) for subset in combinations(freq_item, i)]\n",
    "        print(subsets)\n",
    "        # Here we can see all those that are missing, are the ones that are the consequent\n",
    "    \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3  \n",
    "Support = 4  \n",
    "\n",
    "![title](imgs/Screenshot_6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support = 2  \n",
    "\n",
    "![title](imgs/Screenshot_7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exam sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](imgs/Screenshot_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'A', 'C', 'F', 'G'}, {'A', 'F', 'G', 'H'}, {'B', 'E', 'H', 'I'}]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_3 = [{\"A\", \"B\", \"H\"}, {\"A\", \"C\", \"F\"}, {\"A\", \"C\", \"G\"}, {\"A\", \"E\", \"H\"}, {\"A\", \"F\", \"G\"}, {\"A\", \"F\", \"H\"}, {\"A\", \"G\", \"H\"}, {\"A\", \"H\", \"I\"},\n",
    "       {\"B\", \"E\", \"H\"}, {\"B\", \"E\", \"I\"}, {\"B\", \"F\", \"H\"}, {\"C\", \"F\", \"G\"}, {\"D\", \"E\", \"H\"}, {\"E\", \"H\", \"I\"}, {\"F\", \"H\", \"I\"}]\n",
    "\n",
    "_apriori_join(l_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](imgs/Screenshot_2.png)  \n",
    "\n",
    "Just like the anti-monotone property of support, confidence of rules generated from the same itemset also follows an anti-monotone property. It is anti-monotone with respect to the number of elements in consequent.\n",
    "This means that confidence of (A,B,C→ D) ≥ (B,C → A,D) ≥ (C → A,B,D). To remind, confidence for {X → Y} = support of {X,Y}/support of {X} \n",
    "\n",
    "In taken from the towards data science post. This means that if there are more values on the right hand side, then it will definitely be below the previous one.  \n",
    "\n",
    "\n",
    "![title](imgs/Screenshot_8.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.11 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aa82987a71ca0339a52998cd22613b0c002bda8349fa32a0cb67ceec3936a2bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
